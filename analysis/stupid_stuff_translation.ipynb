{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYmjd7nZt3_A"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Please ensure you have imported a Gemini API key from AI Studio.\n",
        "You can do this directly in the Secrets tab on the left.\n",
        "\n",
        "After doing so, please run the setup cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bclmKMP9t3_O",
        "outputId": "ace3dad6-a920-4243-9400-f50c90a3096d"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google\"\n",
        "!pip install -U -q \"google.genai\"\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from google.colab import drive\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "# Please ensure that uploaded files are available in the AI Studio folder or change the working folder.\n",
        "os.chdir(\"/content/drive/MyDrive/Google AI Studio\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX42burjt3_T"
      },
      "source": [
        "# Generated Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "J2kD1Ui1t3_V",
        "outputId": "06a8d639-fb6a-4269-f751-b61ce94c5941"
      },
      "outputs": [],
      "source": [
        "# To run this code you need to install the following dependencies:\n",
        "# pip install google-genai\n",
        "\n",
        "import base64\n",
        "import os\n",
        "import json\n",
        "from typing import Sequence, Generator\n",
        "\n",
        "from google.genai import Client as GeminiClient\n",
        "from google.genai.types import (\n",
        "    Content,\n",
        "    Part,\n",
        "    SafetySetting,\n",
        "    GenerateContentConfig,\n",
        "    HarmCategory,\n",
        "    HarmBlockThreshold,\n",
        "    FinishReason,\n",
        ")\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "class JokeEntry(BaseModel):\n",
        "    body: str\n",
        "    id: int\n",
        "    rating: float\n",
        "\n",
        "\n",
        "def iter_batches(array: Sequence, batch_size: int = 32, start_from: int = 0) -> Generator[Sequence, None, None]:\n",
        "    for i in range(start_from, len(array), batch_size):\n",
        "        yield array[i:i+batch_size]\n",
        "\n",
        "def translate(batch: list[JokeEntry], prompt: str, generate_content_config: GenerateContentConfig, client: GeminiClient):\n",
        "    contents = [\n",
        "        Content(\n",
        "            role=\"user\",\n",
        "            parts=[\n",
        "                Part.from_text(text=prompt.format(jokes_batch=batch)),\n",
        "            ],\n",
        "        ),\n",
        "    ]\n",
        "    response = client.models.generate_content(\n",
        "        model=model,\n",
        "        contents=contents,\n",
        "        config=generate_content_config,\n",
        "    )\n",
        "    return response\n",
        "\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Google AI Studio/stupid_stuff.json\") as f:\n",
        "    raw_jokes = json.load(f)\n",
        "\n",
        "prompt = \"\"\"Traduce estos chistes al español, para cada objeto del array traduce el texto en la llave body, las demás llaves déjalas intactas.\n",
        "Procura usar un lenguaje que resulte familiar en el contexto latinoamericano.\n",
        "\n",
        "```json\n",
        "{jokes_batch}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "client = GeminiClient(\n",
        "    api_key=os.environ.get(\"GEMINI_API_KEY\"),\n",
        ")\n",
        "model = \"gemini-2.0-flash\"\n",
        "generate_content_config = GenerateContentConfig(\n",
        "    temperature=1,\n",
        "    response_mime_type=\"application/json\",\n",
        "    response_schema=list[JokeEntry],\n",
        "    safety_settings=[\n",
        "        SafetySetting(\n",
        "            category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "            threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "            threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "            threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "            threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "        ),\n",
        "        SafetySetting(\n",
        "            category=HarmCategory.HARM_CATEGORY_CIVIC_INTEGRITY,\n",
        "            threshold=HarmBlockThreshold.BLOCK_NONE,\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "translated = []\n",
        "for i,batch in enumerate(iter_batches(raw_jokes)):\n",
        "    response = translate(batch, prompt, generate_content_config, client)\n",
        "    if response.parsed is None and response.candidates[0].finish_reason in (FinishReason.RECITATION, FinishReason.MAX_TOKENS):\n",
        "        print(f\"batch {i=} with {response.candidates[0].finish_reason} issues\")\n",
        "        # reprocess each joke of the batch separately and isolate the ones with recitation issues\n",
        "        for j, joke in enumerate(batch):\n",
        "            response = translate([joke], prompt, generate_content_config, client)\n",
        "            if response.parsed is None and response.candidates[0].finish_reason == FinishReason.RECITATION:\n",
        "                print(f\"sample with recitation issues, skip and translate later with an alternate approach {joke['id']}\")\n",
        "                continue\n",
        "\n",
        "            translated += response.parsed\n",
        "            print(f\"recitation batch joke {j=}: {len(translated)=}\")\n",
        "\n",
        "    translated += response.parsed\n",
        "    print(f\"batch {i=}: {len(batch)=} {len(translated)=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqR9ZS4H0V_x"
      },
      "outputs": [],
      "source": [
        "translated_raw = [joke.model_dump() for joke in translated]\n",
        "translaated_path = \"/content/drive/MyDrive/Google AI Studio/stupid_stuff_translated.json\"\n",
        "if os.path.exists(translaated_path):\n",
        "    with open(translaated_path) as f:\n",
        "        translated_prior = json.load(f)\n",
        "    translated_raw = translated_prior + translated_raw\n",
        "\n",
        "with open(translaated_path) as f:\n",
        "    json.dump(translated_raw, f, indent=2)\n",
        "\n",
        "print(len(translated_raw), translated_raw[-1][\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkRsswyEXSt5"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
