{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# after uplading the humour-detection.zip and data.zip files\n",
        "%%capture\n",
        "!unzip humor-detection.zip\n",
        "!unzip data.zip\n",
        "!pip install ./humor-detection datasets\n",
        "!pip uninstall wandb -y"
      ],
      "metadata": {
        "id": "jOZx6e6E-5xb"
      },
      "id": "jOZx6e6E-5xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from humor_detection.encoder import (\n",
        "    classification_model,\n",
        "    detection_model,\n",
        "    load_model,\n",
        "    save_model,\n",
        ")\n",
        "from humor_detection.test import test_classification, test_detection\n",
        "from humor_detection.train import train_classification, train_detection\n",
        "from humor_detection.predict import predict_classification, predict_detection\n",
        "from transformers.training_args import TrainingArguments\n",
        "\n",
        "FULL_DATA_TEST = False\n",
        "\n",
        "# Nombre del modelo en HuggingFace\n",
        "model_owner = \"google\"\n",
        "model_name = \"canine-c\"\n",
        "hf_model_id = f\"{model_owner}/{model_name}\"\n",
        "# Ajustes de trainer de Transformers https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments\n",
        "#  Lo más importante es usar bf16 o fp16 para VRAM, batch_sizes para la velocidad y train_epochs para los epochs\n",
        "arguments = TrainingArguments(\n",
        "    bf16=True,\n",
        "    bf16_full_eval=True,\n",
        "    eval_strategy=\"epoch\",\n",
        "    num_train_epochs=2,\n",
        "    optim=\"adamw_8bit\",\n",
        "    per_device_eval_batch_size=150,\n",
        "    per_device_train_batch_size=150,\n",
        "    save_strategy=\"no\"\n",
        ")\n",
        "# Generar el modelo acepta el mismo LoRA usado en PLN  https://huggingface.co/docs/transformers/v4.51.3/en/peft#peft\n",
        "# lora = LoraConfig(\"CAUSAL_LM\", lora_alpha=16, lora_dropout=0.1, r=128)\n",
        "\n",
        "# Prompts por si quieren verificar algo manualmente\n",
        "prompts = [\n",
        "    \"¿Cuál es el último animal que subió al arca de Noé? El del-fin.\",\n",
        "    \"El otro día unas chicas llamarón a mi puerta y me pidieron una pequeña donación para una piscina local.\\nLes di un garrafa de agua.\",\n",
        "    \"The brain surgeon changed my life. He really opened my mind.\",\n",
        "    \"djasndoasndoa\",\n",
        "    \"jajaja\",\n",
        "]\n",
        "\n",
        "# Tarea de clasificación 1 a 5 (Los labels son 0 a 4).\n",
        "def run_classification():\n",
        "    # Función para crear el modelo, tokenizador y añadir un lora si es necesario\n",
        "    model, tokenizer = classification_model(hf_model_id)\n",
        "    # Entrenamiento con datos en español, Con full_dataset=True entrenan el modelo final, english_data=True añade el dataset en inglés\n",
        "    # y prompter permite usar una función para añadir modificar el texto de cada chiste\n",
        "    train_classification(model, tokenizer, arguments, full_dataset=FULL_DATA_TEST)\n",
        "    # Recolección de datos de test con dataset hecho por nosotros\n",
        "    print(test_classification(model, tokenizer, arguments))\n",
        "    path = f\"./models/{model_name}/classification\"\n",
        "    # Función para guardar el modelo\n",
        "    save_model(model, path)\n",
        "    # Función para cargar el modelo\n",
        "    model, _ = load_model(hf_model_id, path)\n",
        "    # Predicción manual de prompts\n",
        "    print(predict_classification(model, tokenizer, prompts, arguments))\n",
        "\n",
        "# Tarea de detección 0 o 1.\n",
        "def run_detection():\n",
        "    model, tokenizer = detection_model(hf_model_id)\n",
        "    train_detection(model, tokenizer, arguments, full_dataset=FULL_DATA_TEST)\n",
        "    print(test_detection(model, tokenizer, arguments))\n",
        "    path = f\"./models/{model_name}/detection\"\n",
        "    save_model(model, path)\n",
        "    model, _ = load_model(hf_model_id, path)\n",
        "    print(predict_detection(model, tokenizer, prompts, arguments))"
      ],
      "metadata": {
        "id": "VlrdfRq18c6X"
      },
      "id": "VlrdfRq18c6X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_classification()\n",
        "run_detection()"
      ],
      "metadata": {
        "id": "Lt-Nhzn08h_F"
      },
      "id": "Lt-Nhzn08h_F",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}